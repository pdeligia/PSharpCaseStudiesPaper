We report our experience of applying \psharp on the three case studies discussed in this paper. We aim to answer the following two questions:

\begin{enumerate}
\item How much human effort was spent in modeling the environment of a distributed system using \psharp?

\item How much computational time was spent in systematically testing a distributed system using \psharp?
\end{enumerate}

\subsection{Cost of environment modeling}
\label{sec:eval:human_cost}

\newcommand{\colspacing}{\hspace{1.8em}}
\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{0.3em}
\begin{tabular}{l rrrrr rr}
\centering
\input{experiments/tables/statistics.tex}
\end{tabular}
\caption{Statistics from modeling the environment of the three \Microsoft MyCloud-based systems under test. The ($\star$) denotes ``awaiting confirmation''.}
\label{tab:stats}
%\vspace{-3mm}
\end{table}

Environment modeling is a core activity of using \psharp. It is required for \emph{closing} a system to make it amenable to systematic testing. Table~\ref{tab:stats} presents program statistics for the three case studies. The columns under ``System'' refer to the real system-under-test, while the columns under ``\psharp Test Harness'' refer to the test harness written in \psharp. We report: lines of code for the system-under-test (\#LoC); number of bugs found in the system-under-test (\#B); lines of \psharp code for the test harness (\#LoC); number of machines (\#M); number of state transitions (\#ST); and number of action handlers (\#AH).

Modeling the environment of the Extent Manager in the \Azure Storage vNext system required approximately two person weeks of part-time developing. The \psharp test harness for this system is the smallest (in lines of code) from the three case studies. This was because this modeling exercise aimed to reproduce the particular liveness bug that was haunting the developers of vNext.

Developing both the MigratingTable system and its \psharp test harness took approximately five person weeks. The \psharp test harness was developed in parallel with the actual system. This is in contrast with the other two case studies discussed in this paper, where the modeling activity occurred independently and after the development process had finished.

Modeling Fabric required approximately five person months, an effort undertaken by the authors of \psharp. In contrast the other two systems discussed in this paper were modeled and tested by their corresponding developers. Although modeling Fabric required a significant amount of time, it is a one-time effort, which only needs incremental refinement with each release.

\subsection{Cost of systematic testing}
\label{sec:eval:machine_cost}

\setlength{\tabcolsep}{.72em}
\begin{table*}[t]
\small
\centering
\begin{tabular}{rl rrr rrr}
\centering
\input{experiments/tables/results.tex}
\end{tabular}
\caption{Results from running the \psharp random and PCT systematic testing schedulers for 100,000 executions. We report: time in seconds to find a bug (Time to Bug); number of scheduling steps when a bug was found (\#SS); and if a bug was found with a particular scheduler (BF?).}
\label{tab:testing}
\end{table*}

Using \psharp we managed to uncover 8 serious bugs in our case studies. As discussed earlier in the paper, these bugs were hard to find with traditional testing techniques, but \psharp managed to uncover them and reproduce them in a small setting. According to the developers, the traces of \psharp were useful, as it allowed them to understand the source of the bug and fix it in a timely manner. After the developers fixed all the discovered bugs, we optionally reintroduced them one-by-one to evaluate the effectiveness of different \psharp systematic testing strategies in finding these bugs.

Table~\ref{tab:testing} presents the results from running the \psharp systematic testing engine on each case study with a re-introduced bug using the random and PCT schedulers. We chose to use controlled random scheduling, because it has proven to be efficient for finding concurrency bugs~\cite{thomson2014sct, deligiannis2015psharp}. The CS column shows which case study corresponds to each bug: ``1'' is for the \Azure Storage vNext; and ``2'' is for MigratingTable. We do not include the Fabric case study, as we are awaiting confirmation of the found bug (see \S\ref{sec:cases:fabric}).

We performed all experiments on a 2.50GHz Intel Core i5-4300U CPU with 8GB RAM running Windows 10 Pro 64-bit. We configured the \psharp testing engine to perform 100,000 executions. The random seed for both schedulers was generated using the current time. The PCT scheduler was further configured with a bug depth of 2 and a max number of scheduling steps of 500. All reported times are in seconds.

For the vNext case study, the random scheduler was able to reproduce the ExtentNodeLivenessViolation bug in less than 10 seconds. The reason that the number of scheduling steps to find the bug is much higher than the rest of the bugs in the table is that this bug is a liveness violation: as discussed in \S\ref{sec:overview:testing} we leave the program to run for a long time before checking if the liveness property holds. The PCT scheduler was unable to find the bug using the bug depth of 2, which suggests that the bug requires a larger depth bound to be found.

For the MigratingTable case study, we evaluated the \psharp test harness of \S\ref{sec:cases:migration} on eleven buggy versions of the system, including eight \emph{organic} bugs that actually occurred in development and three \emph{notional} bugs (denoted by $*$), which are other code changes that we deemed interesting ways of making the system incorrect. The test found seven of the organic bugs, which are listed first in Table~\ref{tab:testing}. The remaining four bugs (denoted by $\diamond$) were not caught with our default test harness in the 100,000 executions (a process that required less than 30 minutes). We believe this is because the inputs and schedules that trigger them are too rare in our distribution. To confirm this, we wrote a custom test case for each bug with a specific input that triggers it and were able to quickly reproduce the bugs, as shown. This method is useful for regression testing, but one could not use it during development with no prior knowledge about the bugs.

The QueryStreamedBackUpNewStream bug in MigratingTable, that was found using \psharp, stands out because it reflects the type of oversight that tends to occur as designs evolve. This bug is in the implementation of a \emph{streaming read} from the virtual table, which should return a stream of all rows in the table sorted by key. The essential implementation idea is to perform streaming reads from both backend tables and merge the results. According to the \texttt{IChainTable} specification, each row read from a stream may reflect the state of the table at any time between when the stream was started and the row was read. The developers sketched a proof that the merging process would preserve this property as long as the migrator was only copying rows from the old table to the new table. But when support was added for the migrator to delete the rows from the old table after copying, it became possible for the backend streams to see the deletion of a row from the old table but not its insertion into the new table, even though the insertion happens first, and the row would be missed. \psharp managed to discover this bug in a matter of seconds. The MigratingTable developers spent just 10 minutes analyzing the trace to diagnose what was happening; although, this was after days of experience analyzing traces and after they had extended the logging of \psharp with additional trace information specific to MigratingTable. \psharp only outputs trace information related to its communicating state machines, but the trace information is easy to extend and was done in all our case studies.

%We started from the failure symptom: the virtual stream returned end-of-stream when according to the reference \texttt{SpecTable}, it should have returned an additional row with key 4.  We filtered the trace for actions by the same service machine and saw that $s_O$ was closed before the virtual stream had returned the row with key 4, but $s_N$ had already advanced past key 4 before the migrator inserted the row in the new table.  At first we found this phenomenon hard to believe, but soon we were convinced it reflected a gap in our design.

%This bug, which we named QueryStreamedBackUpNewStream, is in the implementation of a streaming read from the virtual table, which should return a stream of all rows in the table sorted by key.
%The essential implementation idea is to start streams $s_O$, $s_N$ from the old and new backend tables and merge the sorted streams by keeping track of the next row in each stream and returning the row with the lesser key.  In parallel, the migrator job is concurrently copying rows from the old table to the new table; we had satisfied ourselves that this concurrency would not cause any problems.  However, then we added support to the migrator job to delete the old table when it finishes copying, which triggers the virtual stream to close $s_O$.  Suppose the virtual stream is in a state in which the next row in $s_O$ has key $k_O$ and the next row in $s_N$ has key $k_N$, where $k_O < k_N$.  Further suppose that before the next read from the virtual stream, the migrator job copies a row with key $k$ ($k_O < k < k_N$) from the old table to the new table and then deletes the old table.  Since $s_O$ has not yet returned this row when it is closed and $s_N$ has already advanced to $k_N$, the row with key $k$ will be missed by the virtual stream.  A similar problem can occur if $s_N$ does not reflect rows inserted into the new table by the migrator job after $s_N$ is started, as allowed by the \texttt{IChainTable} specification.  Restarting $s_N$ when the old table is deleted fixes both variants of the bug.

% It might be nice to include excerpts of a trace like in migration-bug3-explanation.pptx.  Unfortunately, the trace in migration-bug3-explanation.pptx doesn't match my recollection of the original diagnosis, which I want to write about truthfully (the former looks like it involves a stale streaming read, while I'm fairly sure the latter involved only the $k_O < k < k_N$ case).  If it's important, I could try to get a new trace consistent with the original diagnosis. ~ Matt

%This bug took us only about 10 minutes to diagnose from the trace; granted, this is after we had days of experience analyzing MigratingTable traces and had added our own trace output to the test harness, since \psharp's built-in trace output is too low-level and does not include event payloads and other diagnostic data that is not passed between machines.

%We started from the failure symptom: the virtual stream returned end-of-stream when according to the reference \texttt{SpecTable}, it should have returned an additional row with key 4.  We filtered the trace for actions by the same service machine and saw that $s_O$ was closed before the virtual stream had returned the row with key 4, but $s_N$ had already advanced past key 4 before the migrator inserted the row in the new table.  At first we found this phenomenon hard to believe, but soon we were convinced it reflected a gap in our design.
