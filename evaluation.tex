We report our experience of applying \psharp on the three case studies discussed in this paper. We aim to answer the following two questions:

\begin{enumerate}
\item How much human effort was spent in modeling the environment of a distributed system using \psharp?

\item How much computational time was spent in systematically testing a distributed system using \psharp?
\end{enumerate}

\subsection{Cost of environment modeling}
\label{sec:eval:human_cost}

\newcommand{\colspacing}{\hspace{1.8em}}
\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{0.3em}
\begin{tabular}{l rrrrr rr}
\centering
\input{experiments/tables/statistics.tex}
\end{tabular}
\caption{Statistics from modeling the environment of the three Microsoft Azure-based systems under test. The ($\star$) denotes ``awaiting confirmation''.}
\label{tab:stats}
%\vspace{-3mm}
\end{table}

Environment modeling is a core activity of using \psharp. It is required for \emph{closing} a system to make it amenable to systematic testing. Table~\ref{tab:stats} presents program statistics for the three case studies. The columns under ``System'' refer to the real system-under-test, while the columns under ``\psharp model'' refer to the \psharp test harness. We report: lines of code for the system-under-test (\#LoC); number of bugs found in the system-under-test (\#B); lines of \psharp code for the test harness (\#LoC); number of machines (\#M); number of state transitions (\#ST); and number of action handlers (\#AH).

Modeling the environment of the Extent Manager in the Azure Storage vNext system required approximately two weeks of part-time developing. The \psharp model for testing this system is the smallest (in lines of code) from the three case studies. This was because this modeling exercise aimed to reproduce the particular liveness bug that was haunting the developers of vNext.

Developing the Live Migration Table and its \psharp test harness took about five weeks. The test harness was developed in parallel with the actual system. This is in contrast with the other two case studies discussed in this paper, where the modeling activity occurred independently and at a later stage of the development process.

Modeling Fabric required approximately five person months. Although this is a significant amount of time, it is a one-time effort, which only needs incremental refinement with each release.

\subsection{Cost of systematic testing}
\label{sec:eval:machine_cost}

\setlength{\tabcolsep}{.72em}
\begin{table*}[t]
\small
\centering
\begin{tabular}{rl rrr rrr}
\centering
\input{experiments/tables/results.tex}
\end{tabular}
\caption{Results from running the \psharp random and PCT systematic testing schedulers for 100,000 iterations. We report: time in seconds to find a bug (Time to Bug); number of scheduling steps when a bug was found (\#SS); and if a bug was found with a particular scheduler (BF?).}
\label{tab:testing}
\end{table*}

Using \psharp we managed to uncover 12 serious bugs in our case studies \PDComment{update for Fabric}. As discussed earlier in the paper, these bugs were hard to find with traditional testing techniques, but \psharp managed to uncover them and reproduce them in a small setting. According to the developers, the traces of \psharp were useful, as it allowed them to understand the source of the bug and fix it in a timely manner. After the developers fixed all the discovered bugs, we optionally reintroduced them one-by-one to evaluate the effectiveness of different \psharp systematic testing strategies in finding these bugs.

Table~\ref{tab:testing} presents the results from running the \psharp systematic testing engine on each case study with a re-introduced bug using the random and PCT schedulers. We chose to use controlled random scheduling, because it has proven to be efficient for finding concurrency bugs~\cite{thomson2014sct, deligiannis2015psharp}. The CS column shows which case study corresponds to each bug: 1 is for the Azure Storage vNext; and 2 is for the Live Migration Table. \PDComment{update with 3 for Fabric}

We performed all experiments using the Windows PowerShell tool on a 2.50GHz Intel Core i5-4300U CPU with 8GB RAM running Windows 10 Pro 64-bit. We configured the engine to perform 100,000 iterations. The random seed for both schedulers was generated using the current time. The PCT scheduler was further configured with a bug depth of 2 and a max number of scheduling steps of 500. All reported times are in seconds.

For the vNext case study, the random scheduler was able to reproduce the bug in less than 10 seconds. The reason that the number of scheduling steps to find the bug is much higher than the rest of the bugs in the table is that this bug is a liveness violation: as discussed in \S\ref{sec:psharp:testing} we leave the program to run for a long time before checking if the liveness property holds. The PCT scheduler was unable to find the bug using the bug depth of 2, which suggests that the bug requires a larger depth bound to be found.

For the MigratingTable case study, the first seven bugs in Table~\ref{tab:testing} were discovered using a \psharp test harness based on nondeterministically generated, but controlled by \psharp, input history. For each of these found bugs, we manually reviewed one of the bug traces to confirm if it reflected the expected bug. The remaining four bugs (denoted with $\diamond$) were not caught with our default test harness in the 100,000 iterations (a process that required less than 30 minutes). We believe this is due to unlucky random choices of inputs and schedules by the \psharp testing engine. We wrote a custom \psharp test harness for each of these bugs, which allowed \psharp to quickly reproduce them. The design of these custom harnesses was influenced by the knowledge of these bugs; our objective here was to simply see if \psharp can reproduce these known bugs.

The QueryStreamedBackUpNewStream bug in MigratingTable, that was found using \psharp, stands out because it reflects the type of oversight that tends to occur as designs evolve. We do not discuss the details of the bug due to space constraints, but \psharp managed to discover this bug in a matter of seconds. The MigratingTable developers spent just 10 minutes analyzing the trace to diagnose what was happening; although, this was after days of experience analyzing traces. The developers extended the logging of \psharp with additional trace information to understand the bug. \psharp only outputs trace information related to its communicating state machines, but the trace information is easy to extend and was done in all our case studies.

%We started from the failure symptom: the virtual stream returned end-of-stream when according to the reference \texttt{SpecTable}, it should have returned an additional row with key 4.  We filtered the trace for actions by the same service machine and saw that $s_O$ was closed before the virtual stream had returned the row with key 4, but $s_N$ had already advanced past key 4 before the migrator inserted the row in the new table.  At first we found this phenomenon hard to believe, but soon we were convinced it reflected a gap in our design.

%This bug, which we named QueryStreamedBackUpNewStream, is in the implementation of a streaming read from the virtual table, which should return a stream of all rows in the table sorted by key.
%The essential implementation idea is to start streams $s_O$, $s_N$ from the old and new backend tables and merge the sorted streams by keeping track of the next row in each stream and returning the row with the lesser key.  In parallel, the migrator job is concurrently copying rows from the old table to the new table; we had satisfied ourselves that this concurrency would not cause any problems.  However, then we added support to the migrator job to delete the old table when it finishes copying, which triggers the virtual stream to close $s_O$.  Suppose the virtual stream is in a state in which the next row in $s_O$ has key $k_O$ and the next row in $s_N$ has key $k_N$, where $k_O < k_N$.  Further suppose that before the next read from the virtual stream, the migrator job copies a row with key $k$ ($k_O < k < k_N$) from the old table to the new table and then deletes the old table.  Since $s_O$ has not yet returned this row when it is closed and $s_N$ has already advanced to $k_N$, the row with key $k$ will be missed by the virtual stream.  A similar problem can occur if $s_N$ does not reflect rows inserted into the new table by the migrator job after $s_N$ is started, as allowed by the \texttt{IChainTable} specification.  Restarting $s_N$ when the old table is deleted fixes both variants of the bug.

% It might be nice to include excerpts of a trace like in migration-bug3-explanation.pptx.  Unfortunately, the trace in migration-bug3-explanation.pptx doesn't match my recollection of the original diagnosis, which I want to write about truthfully (the former looks like it involves a stale streaming read, while I'm fairly sure the latter involved only the $k_O < k < k_N$ case).  If it's important, I could try to get a new trace consistent with the original diagnosis. ~ Matt

%This bug took us only about 10 minutes to diagnose from the trace; granted, this is after we had days of experience analyzing MigratingTable traces and had added our own trace output to the test harness, since \psharp's built-in trace output is too low-level and does not include event payloads and other diagnostic data that is not passed between machines.

%We started from the failure symptom: the virtual stream returned end-of-stream when according to the reference \texttt{SpecTable}, it should have returned an additional row with key 4.  We filtered the trace for actions by the same service machine and saw that $s_O$ was closed before the virtual stream had returned the row with key 4, but $s_N$ had already advanced past key 4 before the migrator inserted the row in the new table.  At first we found this phenomenon hard to believe, but soon we were convinced it reflected a gap in our design.
