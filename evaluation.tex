We report our experience of applying \psharp on the three case studies discussed in this paper. We aim to answer the two following questions:

\begin{enumerate}
\item How much human effort was spent in modeling the environment of a distributed system using \psharp?

\item How much computational time was spent in systematically testing a distributed system using \psharp?
\end{enumerate}

\subsection{Cost of environmental modeling}
\label{sec:eval:human_cost}

\newcommand{\colspacing}{\hspace{1.8em}}
\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{0.3em}
\begin{tabular}{l rrrrr rr}
\centering
\input{experiments/tables/statistics.tex}
\end{tabular}
\caption{Statistics from modeling the environment of the three Microsoft Azure-based systems under test.}
\label{tab:stats}
%\vspace{-3mm}
\end{table}

Environmental modeling is a core activity of using \psharp. It is required for \emph{closing the environment} of a system under test and making it amenable to systematic testing. Table~\ref{tab:stats} presents program statistics for our three case studies. We report: lines of code for the system under test (\#LoC); number of bugs found in the system (\#B); lines of \psharp code for the environmental model (\#LoC); number of machines (\#M); number of state transitions (\#ST); and number of action handlers (\#AH).

Modeling the environment of the Extent Manager in the Azure Storage vNext system required approximately 2 weeks of part-time developing. The \psharp model for testing this system is the smallest (in lines of code) from all three case studies. This was because the modeling effort was targeting the particular liveness bug that was haunting the developers of vNext. We are currently in the process of modeling other components of vNext, such as a ChainReplication and a Paxos system.

Modeling the Live Migration Table required \PDComment{waiting confirmation from Matt}. This case study is interesting because the development of the actual system and its \psharp environmental model occurred side-by-side. This is in contrast with the other two case studies discussed in this paper, where the modeling activity occurred independently and at a later stage of the development process.

Modeling Fabric required approximately 4-5 months. Although this is a significant amount of time, it is a one time effort activity. Our plan is to reuse the developed Fabric model for testing arbitrary user services built for the Azure Service Fabric system.

\subsection{Cost of systematic testing}
\label{sec:eval:machine_cost}

We then present runtime results of using \psharp with two different systematic testing schedulers to find bugs in the case studies.

After we fixed all the bugs we could find in the programs under test, we added options to the code to introduce various benchmark bugs (some that were actually in the original programs and some we made up) one at a time so we could evaluate different methods to detect and diagnose them.

After fixing all the bugs we found in MigratingTable, we added an option to conditionally reintroduce each of the following bugs.

We performed all experiments using the Windows PowerShell tool on a 2.50GHz Intel Core i5-4300U CPU with 8GB RAM running Windows 10 Pro 64-bit.

\setlength{\tabcolsep}{.72em}
\begin{table*}[t]
\small
\centering
\begin{tabular}{rl rrr rrr}
\centering
\input{experiments/tables/results.tex}
\end{tabular}
\caption{Results from running the \psharp random and PCT systematic testing schedulers for 100,000 iterations. We report: time in seconds to find a bug (Time to Bug); number of scheduling steps when a bug was found (\#SS); and if a bug was found with a particular scheduler (BF?).}
\label{tab:testing}
\end{table*}

Table~\ref{tab:testing} presents the results from running the \psharp systematic testing engine on each case study with an enabled bug using the random and the PCT schedulers. We configured the engine to perform 100,000 iterations. The random seed for both schedulers was generated in each iteration using the \texttt{DateTime.Now.Millisecond} API which represents the current time in milliseconds. The PCT scheduler was configured with a bug depth of 2 and a max number of scheduling steps to execute of 500. All reported times are in seconds.

For MigratingTable, the upper section of the table uses the random input generator described in Section~\ref{sec:mtable:input}.  For each bug that led to at least one test failure, we manually reviewed one of the failure traces to confirm it reflected the intended bug.  For each of the remaining bugs, we repeated the test using a test case custom written to trigger that particular bug in order to confirm that the failure to detect the bug in the original test was due to unlucky random choices of inputs and schedules and not some other problem with the experimental setup.  These results are in the lower section of the table; they can serve as additional cases in which to compare the random and PCT schedulers but do not represent a testing method one could use to find unknown bugs in software.  It may have been interesting to report results for specific test cases written before we knew the bugs as mentioned in \ref{sec:mtable:input}, but we did not do so in this study.

Controlled random scheduling has proven to be efficient for finding concurrency bugs~\cite{thomson2014sct, deligiannis2015psharp}.
