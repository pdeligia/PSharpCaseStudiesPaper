We report our experience of applying \psharp on the three case studies discussed in this paper. We aim to answer the two following questions:

\begin{enumerate}
\item How much human effort was spent in modeling the environment of a distributed system using \psharp?

\item How much computational time was spent in systematically testing a distributed system using \psharp?
\end{enumerate}

\subsection{Cost of environmental modeling}
\label{sec:eval:human_cost}

\newcommand{\colspacing}{\hspace{1.8em}}
\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{0.3em}
\begin{tabular}{l rrrrr rr}
\centering
\input{experiments/tables/statistics.tex}
\end{tabular}
\caption{Statistics from modeling the environment of the three Microsoft Azure-based systems under test.}
\label{tab:stats}
%\vspace{-3mm}
\end{table}

Environmental modeling is a core activity of using \psharp. It is required for \emph{closing the environment} of a system under test and making it amenable to systematic testing. Table~\ref{tab:stats} presents program statistics for our three case studies. We report: lines of code for the system under test (\#LoC); number of bugs found in the system (\#B); lines of \psharp code for the environmental model (\#LoC); number of machines (\#M); number of state transitions (\#ST); and number of action handlers (\#AH).

Modeling the environment of the Extent Manager in the Azure Storage vNext system required approximately 2 weeks of part-time developing. The \psharp model for testing this system is the smallest (in lines of code) from all three case studies. This was because the modeling effort was targeting the particular liveness bug that was haunting the developers of vNext. We are currently in the process of modeling other components of vNext, such as a ChainReplication and a Paxos system.

Modeling the Live Migration Table required \PDComment{waiting confirmation from Matt}. This case study is interesting because the development of the actual system and its \psharp environmental model occurred side-by-side. This is in contrast with the other two case studies discussed in this paper, where the modeling activity occurred independently and at a later stage of the development process.

Modeling Fabric required approximately 4-5 months. Although this is a significant amount of time, it is a one time effort activity. Our plan is to reuse the developed Fabric model for testing arbitrary user services built for the Azure Service Fabric system.

\subsection{Cost of systematic testing}
\label{sec:eval:machine_cost}

\setlength{\tabcolsep}{.72em}
\begin{table*}[t]
\small
\centering
\begin{tabular}{rl rrr rrr}
\centering
\input{experiments/tables/results.tex}
\end{tabular}
\caption{Results from running the \psharp random and PCT systematic testing schedulers for 100,000 iterations. We report: time in seconds to find a bug (Time to Bug); number of scheduling steps when a bug was found (\#SS); and if a bug was found with a particular scheduler (BF?).}
\label{tab:testing}
\end{table*}

Using \psharp we managed to uncover more than 10 serious bugs in our case studies. As discussed earlier in the paper, these bugs were hard to find with traditional testing techniques, but \psharp managed to uncover them and reproduce them in a small setting. According to the developers, the traces of \psharp were useful, as it allowed them to understand the source of the bug and fix it in a timely manner. After the developers fixed all the discovered bugs, we optionally reintroduced them one-by-one so that we can evaluate the effectiveness of different \psharp systematic testing strategies in finding these bugs.

Table~\ref{tab:testing} presents the results from running the \psharp systematic testing engine on each case study with a re-introduced bug using the random and the PCT schedulers. The CS column shows which case study corresponds to each bug: 1 is for the Azure Storage vNext; and 2 is for the Live Migration Table. \PDComment{update with 3 for Fabric}

We performed all experiments using the Windows PowerShell tool on a 2.50GHz Intel Core i5-4300U CPU with 8GB RAM running Windows 10 Pro 64-bit. We configured the engine to perform 100,000 iterations. The random seed for both schedulers was generated in each iteration using the \texttt{DateTime.Now.Millisecond} API which returns the current time in milliseconds. The PCT scheduler was further configured with a bug depth of 2 and a max number of scheduling steps to execute of 500. All reported times are in seconds.

For the vNext case study, the random scheduler was able to reproduce the bug in less than 10 seconds. The reason that the number of scheduling steps to find the bug is much higher than the rest of the bugs in the table is that this bug is a liveness violation: as discussed in Section~\ref{} we leave the program to run for a long time before checking if the liveness property holds. The PCT scheduler was unable to find the bug using the bug depth of 2, which suggests that the bug requires a larger depth bound to be found.

For MigratingTable, the upper section of the table uses the random input generator described in Section~\ref{sec:mtable:input}.  For each bug that led to at least one test failure, we manually reviewed one of the failure traces to confirm it reflected the intended bug.  For each of the remaining bugs, we repeated the test using a test case custom written to trigger that particular bug in order to confirm that the failure to detect the bug in the original test was due to unlucky random choices of inputs and schedules and not some other problem with the experimental setup.  These results are in the lower section of the table; they can serve as additional cases in which to compare the random and PCT schedulers but do not represent a testing method one could use to find unknown bugs in software.  It may have been interesting to report results for specific test cases written before we knew the bugs as mentioned in \ref{sec:mtable:input}, but we did not do so in this study. \PDComment{this paragraph needs to be trimmed down probably}

Controlled random scheduling has proven to be efficient for finding concurrency bugs~\cite{thomson2014sct, deligiannis2015psharp}.
