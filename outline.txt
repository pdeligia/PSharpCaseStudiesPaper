	Introduction
		Well-publicized failures of well-known systems
			See TLA experience article?
		Value of testing the same code used in production **
			Test / production implementations of interfaces (same as in traditional automated testing)
		Remind people that the bug-finding scheduler produces a sequential trace across machines.
	Motivate the class of bugs in fault-tolerant distributed storage systems that our technique aims to find
		Cheng's bug.  Easy to think you can do better until it happens to you. [Pantazis to draft]
		(Cite existing papers?)
	Each case study.  Briefly what the system does and the modeling approach.
		Cheng's case (master only)
		Cheng's case (chain replication with async code), if ready
		Matt's case (Matt's custom async implementation) [Matt to draft]
	Evaluation
		Evidence that we can find a certain class of bugs.
			Cheng's case (master only): actual bug
			Matt's case: migrator stream bug (assuming we can fix the verification to find it) [Matt TODO]
		Compare traditional "stress test" to P# bug-finding scheduler in terms of (1) time to hit a bug and (2) trace length the developer has to look at.
			John to put together an appropriate "stress test" for the migration case study?
	Best practice recommendations for modeling?
	"Wait for event" a useful addition to the programming model.  (So you can test what you want to execute.)  Does it fit in paper?
		Tax on understandability of traces?




Environment can be modelled using the message passing language
Connection between environment and code to be tested through dynamic dispatch and interfaces

Contributions:

The state of the art is TLA+ but there you have environmental model and the model of the system, and you test them, but what you test is the model and not the code that actually executes (the TLA+ models are "thrown away").

We can test/verify C# which is a mainstream language

Env Model (box) + Code uner test (box)

What we do is concurrency unit testing / modist is doing full system testing --> there is a tradeoff, increasing responsibility of modelling but can make stuff simpler, can find bugs in simpler situations (traces)

We can use any combination of real + fake components

Vs Chess ->>> chess did not provide modelling capability, had no real functionality to isolate components and achieve real concurrency unit testing

now we have systematic concurrency testing (systematic cause we control the scheduler)


What to say about intra-machine concurrency (Matt & Pantazis 2015-08-19):
- If there is none, then P# finds all behaviors out of the box (theorem).

- If there is, then it's the programmer's responsibility to implement the concurrency mechanism
in terms of P# message passing.  At a minimum, need to avoid deadlock somehow.  Beyond that,
better implementations will allow P# to find more behaviors. Because we use under-approximation anyway,
its fine as we still find bugs, its just that if we add more behaviors, then we get more chances to
find extra bugs.

  - In general, one can change the code to use an interface that has different production and
P#-testing implementations.  This has some cost (Matt: we shouldn't claim it's unreasonable to hope
that future work might avoid this cost, but only that our work compares favorably to previous work
overall).  There's some precedent for this in using a mock clock for programs that sleep in production.

  - For the migration case study, it's relatively easy because TPL lets us substitute a custom task
scheduler.  There are some behaviors this won't find (certain interleavings of table operations and
configuration pushes), but it still worked well in evaluation and did find one bug involving an
interleaving between table operations and configuration pushes.  A similar approach may work for other programs.
