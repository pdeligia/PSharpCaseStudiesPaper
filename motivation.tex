Microsoft Azure Storage is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data. It has grown from 10s of Petabytes (PB) in 2010 to Exabytes (EB) in 2015, with the total number of objects stored well-exceeding 60 trillion.

Azure Storage vNext is the next generation storage system for Microsoft Azure, where the primary design target is to increase the scalability by 100x or more. Similar to the current system, vNext employs containers, called {\em extents}, to store data. Extents are typically several Gigabytes each, consisting of many data blocks, and replicated over multiple {\em Extent Nodes} (ENs). In contrast to the current system, which employs a Paxos-based, centralized mapping from extents to ENs, vNext achieves its scalability target by employing a completely distributed mapping. In vNext, extents are divided into partitions, with each partition managed by a light-weight {\em Extent Manager} (ExtMgr).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{img/azurestore}
\caption{Top-level components of a distributed extent management system for Windows Azure.}
\label{fig:azurestore}
\end{figure}

One of the many responsibilities of ExtMgr is to ensure that every extent maintains enough replicas in the system. To achieve this, ExtMgr receives frequent periodic \emph{heartbeat} messages from every EN. Failure of EN is detected by missing heartbeats. ExtMgr also receives less frequent, but still periodic {\em synchronization reports} from every EN. The sync reports list all the extents (and associated metadata) stored on the EN. Based on these two types of messages, ExtMgr identifies which ENs have failed and which extents are affected and missing replicas. ExtMgr, then, schedules tasks to repair the affected extents and distributes the tasks to ENs. ENs repair the extents from their existing replicas in the system and lazily update ExtMgr via future sync reports.

To ensure correctness, the developers of vNext have instrumented extensive, multiple levels of testing for Extent Manager:
\begin{enumerate}
\item \emph{Unit testing}, which sends emulated heartbeats and sync reports to ExtMgr and verifies that the messages are processed as expected.

\item \emph{Integration testing}, which launches an ExtMgr together with multiple ENs, subsequently injects an EN failure, and finally verifies that the affected extents are eventually repaired.

\item \emph{Stress testing}, which launches an ExtMgr with multiple ENs and multiple extents. It keeps repeating the following process: injecting an EN failure, launching a new EN and verifying that the affected extents are eventually repaired.
\end{enumerate}

Despite of the extensive testing efforts, the vNext developers have been plagued by what appears to be an elusive bug in ExtMgr. All the unit test and integration test suites successfully pass every single time. However, the stress test suite could fail {\em from time to time} after very long executions, manifested as the replicas of some extents remain missing while never being repaired. The bug appears difficult to identify, reproduce and troubleshoot. First, it takes very long executions to trigger. Second, an extent not being repaired is {\em not} a property that can be easily verified. In practice, the developers rely on very large time-out period to detect the bug. Finally, by the time that the bug is detected, very long execution traces have been collected, which makes manual inspection tedious and ineffective.

To uncover this bug and many other similar ones, the developers are in constant search of a generic and systematic approach for testing distributed storage systems.

%Since this is a rather typical dilemma in the development of distributed storage systems, a general and systematic approach hopefully would be very helpful to the developers and greatly increase their productivity.
