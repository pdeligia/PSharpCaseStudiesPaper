Distributed systems are notoriously hard to design, implement and test~\cite{cavage2013there, leesatapornwongsa2014samc, gunawi2014cbs, laguna2015debugging, maddox2015test}. This challenge is due to many sources of \emph{nondeterminism}~\cite{chandra2007paxos, henry2009cloud, leesatapornwongsa2016taxdc}, such as unexpected node failures, the asynchronous interaction between system components, data losses due to unreliable communication channels, the use of multithreaded code to exploit multicore machines, and interaction with clients. All these sources of nondeterminism can easily create \emph{Heisenbugs}~\cite{gray1986computers, musuvathi2008finding}, corner-case bugs that are difficult to detect, diagnose and fix. These bugs might hide inside a code path that can only be triggered by a specific interleaving of concurrent events and only manifest under extremely rare conditions~\cite{gray1986computers, musuvathi2008finding}, but the consequences can be catastrophic~\cite{amazon2012aws, google2014outage}.

Developers of production distributed systems use many testing techniques, such as unit testing, integration testing, stress testing, and fault injection. In spite of extensive use of these testing methods, many bugs that arise from subtle combinations of concurrency and failure events are missed during testing and get exposed only in production. However, allowing serious bugs to reach production can cost organizations a lot of money~\cite{tassey2002economic} and lead to customer dissatisfaction~\cite{amazon2012aws, google2014outage}.

We interviewed technical leaders and senior managers in Microsoft Azure regarding the top problems in distributed system development. The consensus was that one of the most critical problems today is how to improve \emph{testing coverage} so that bugs can be uncovered \emph{during testing} and \emph{not in production}. The need for better testing techniques is not specific to Microsoft; other companies, such as Amazon and Google, have acknowledged~\cite{chandra2007paxos,newcombe2015aws} that testing methodologies have to improve to be able to reason about the correctness of increasingly more complex distributed systems that are used in production.

Recently, the Amazon Web Services (AWS) team used formal methods ``to prevent serious but subtle bugs from reaching production''~\cite{newcombe2015aws}. The gist of their approach is to extract the high-level logic from a production system, represent this logic as specifications in the expressive TLA+~\cite{lamport1994temporal} language, and finally verify the specifications using a model checker. While highly effective, as demonstrated by its use in AWS, this approach falls short of ``verifying that executable code correctly implements the high-level specification'''~\cite{newcombe2015aws}, and the AWS team admits that it is ``not aware of any such tools that can handle distributed systems as large and complex as those being built at Amazon''~\cite{newcombe2015aws}.

We have found that checking high-level specifications is necessary but not sufficient, due to the gap between the specification and the executable code. Our goal is to bridge this gap. We propose a new methodology that validates high-level specifications directly on the executable code. Our methodology is different from prior approaches that required developers to either switch to an unfamiliar domain specific language~\cite{killian2007life, desai2015building}, or manually annotate and instrument their code~\cite{simsa2011dbug}. Instead, we allow developers to systematically test \emph{production code} by writing test harnesses in \csharp, a mainstream programming language. This significantly lowered the acceptance barrier for adoption by the Microsoft Azure team.

Our testing methodology is based on \psharp~\cite{deligiannis2015psharp}, an extension of \csharp that provides support for modeling, specification, and systematic testing of distributed systems written in the Microsoft .NET framework. To use \psharp for testing, the programmer has to augment the original system with three artifacts: a model of the nondeterministic execution environment of the system; a test harness that drives the system towards interesting behaviors; and safety or liveness specifications. \psharp then systematically exercises the test harness and validates program behaviors against the provided specifications.

The original \psharp paper~\cite{deligiannis2015psharp} discussed language design issues and data race detection for programs written in \psharp, whereas this work focuses on using \psharp to test three distributed storage systems inside Microsoft: Azure Storage vNext; Live Table Migration; and Azure Service Fabric. We uncovered numerous bugs in these systems, including a subtle liveness bug that only intermittently manifested during stress testing for months without being fixed. Our testing approach uncovered this bug in a very small setting, which made it easy for developers to examine traces, identify, and fix the problem.

To summarize, our contributions are as follows:
\begin{itemize}
\item We present a new methodology for modeling, specifying properties of correctness, and systematically testing real distributed systems with \psharp.
\item We discuss our experience of using \psharp to test three distributed storage systems built on top of Microsoft Azure, finding subtle bugs that could not be found with traditional testing techniques.
\item We evaluate the cost and benefits of using our approach, and show that \psharp can detect bugs in a small setting and with easy to understand traces.
\end{itemize}
