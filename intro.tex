Distributed systems are notoriously hard to design, implement and test~\cite{cavage2013there, laguna2015debugging, maddox2015test}. This challenge is due to many well-known sources of \emph{nondeterminism}~\cite{chandra2007paxos}, such as race conditions in the asynchronous interaction between system components, the use of multithreaded code inside components, unexpected node failures, data losses due to unreliable communication channels, and interaction with (human) clients. All these sources of nondeterminism can easily create \emph{Heisenbugs}~\cite{gray1986computers, musuvathi2008finding}, corner-case bugs that are difficult to detect, diagnose and fix. A Heisenbug might hide deep inside one of these paths and only manifest under extremely rare conditions~\cite{gray1986computers, musuvathi2008finding}, but the consequence can be catastrophic~\cite{amazon2012aws, google2014outage}.

Developers of production distributed systems use many testing techniques, such as unit testing, integration testing, stress testing, and fault injection. In spite of extensive use of these testing methods, many bugs that arise from subtle combinations of concurrency and failure events are missed during testing and get exposed only after a system has been put into production. However, allowing bugs to reach production, before they are found and fixed, can cost organizations a lot of money~\cite{tassey2002economic} and lead to customer dissatisfaction~\cite{amazon2012aws, google2014outage}.

We interviewed technical leaders and senior managers in the Microsoft Azure team regarding the top problems in distributed system development. The consensus was that one of the most critical problems today is how to improve \emph{testing coverage} to find bugs \emph{before} a system goes to production. The need for better testing techniques is not specific to Microsoft; other companies, such as Amazon and Google, have acknowledged~\cite{chandra2007paxos,newcombe2015aws} that testing methodologies have to improve to be able to reason about the correctness of increasingly more complex distributed systems that are used in production.

Recently, engineers in Amazon Web Services (AWS) described their use of formal methods ``to prevent serious but subtle bugs from reaching production''~\cite{newcombe2015aws}. The gist of their approach is to extract the high-level logic from a production system, represent this logic as specifications in the expressive TLA+~\cite{lamport1994temporal} language, and finally verify the specifications using a model checker. While highly effective, as demonstrated by its use in AWS, this approach falls short of ``verifying that executable code correctly implements the high-level specification'', and the AWS team admits that ``we are not aware of any such tools that can handle distributed systems as large and complex as those being built at Amazon''~\cite{newcombe2015aws}.

We have found that checking high-level specifications is necessary but not sufficient, due to the gap between the specification and the executable code. Our goal is to bridge this gap. We propose a new methodology that validates high-level specifications directly on the executable code. Our methodology is different from prior approaches that required developers to either switch to an unfamiliar domain specific language~\cite{killian2007life, desai2015building}, or manually annotate and instrument their code~\cite{simsa2011dbug}. Instead, we allow developers to test \emph{production code} by writing test harnesses in \csharp, a mainstream programming language. This significantly lowered the acceptance barrier for adoption by the Microsoft Azure team.

%We present a new method for testing an implementation of a distributed system and uncovering bugs before the system is released in the wild.
Our methodology is based on \psharp~\cite{deligiannis2015psharp}, an extension of \csharp that provides support for modeling, specification, and systematic testing of distributed systems written in Microsoft's .NET framework. To test a distributed system with \psharp, the programmer augments the original system code with three artifacts -- a model of the nondeterministic execution environment of the system, a concurrent test harness that drives the system towards interesting behaviors, and safety or liveness specifications.
The \psharp testing engine then systematically explores all behaviors exercised by the test harness and validates them against the provided specifications.

We have applied \psharp to three distributed storage systems inside Microsoft: Azure Storage vNext; Azure Table Live Migration; and Azure Service Fabric.
%a distributed storage system; a system for migrating data across partitions; and a big-data computing system.
We uncovered numerous bugs in these systems, including, most notably, a subtle liveness bug that only intermittently manifested during stress testing for months without being fixed.
Our testing approach uncovered this bug in a very small setting, which made it easy for developers to examine traces, identify, and fix the problem.
%Our experience demonstrates that bugs in production do not need a large setting to be reproduced; they can be reproduced in small settings with easy to understand traces.

To summarize, our contributions are as follows:
\begin{itemize}
\item We present a new methodology that allows flexible modeling of the environment of a distributed system using simple language mechanisms in \psharp.
\item Because \psharp is an extension of \csharp, our infrastructure can test production code written in \csharp.
\item We present three case studies of using \psharp to test production distributed systems, finding bugs that could not be found with traditional testing techniques.
\item We show that \psharp can reproduce bugs in production-scale distributed systems in a small setting with easy to understand traces.
\end{itemize}

