Distributed systems are notoriously hard to design, implement and test~\cite{cavage2013there, laguna2015debugging, maddox2015test}. This is due to many well-known sources of \emph{nondeterminism}~\cite{chandra2007paxos}, such as race conditions in the asynchronous interaction between system components, the use of multithreaded code inside a component, unexpected node failures, unreliable communication channels and data losses, and interaction with (human) clients.
%
All these sources of nondeterminism translate into \emph{exponentially} many execution paths that a distributed system might potentially execute. A bug might hide deep inside one of these paths and only manifest under extremely rare corner cases~\cite{gray1986computers, musuvathi2008finding}.\SCComment{I suggest we add "but the consequence can be catastrophic". Otherwise, why does such an extremely unlikely bug matter?}

Classic techniques that product groups employ to test, verify and validate their systems (such as code reviews, unit testing, stress testing and fault injection) are unable to capture and control all the aforementioned sources of nondeterminism \SCComment{my comment is similar to the one in the intro: do we claim that we capture all sources of nondet?}, which causes the most tricky bugs being missed during testing and only getting exposed after a system has been put in production. Discovering and fixing bugs in production, though, is bad for business as it can cost a lot of money and many dissatisfied customers. \SCComment{do we have a few real stories about the damages? They are helpful.} 

We interviewed engineers from the Microsoft Azure team regarding the top problems in distributed system development, and the unified response \SCComment{change "unified response" to "consensus"?} was that the most critical problem today is how to improve \emph{testing coverage} to find bugs \emph{before} a system goes in production. The need for better testing techniques is not specific to Microsoft; other companies, such as Amazon and Google, publicly acknowledge~\cite{newcombe2015aws} that testing methodologies have to improve to be able to reason about the correctness of increasingly more complex distributed systems.

Amazon recently published an article~\cite{newcombe2015aws} that describes their use of TLA+~\cite{lamport1994temporal} to detect distributed system bugs and prevent them from reaching production. TLA+ is a powerful specification language for verifying distributed protocols, but it is unable to verify the code that is actually being executed. The implied assumption is that a model of the system will be verified, and then the programmers are responsible to match what was verified with the source code of the real system. Although many design bugs can be caught with this approach, there is \emph{no guarantee} that the real distributed system will be free of bugs.\SCComment{we want to avoid making a general point that TLA+ has no guarantee to find all bugs, because our approach cannot either. Instead, we should say something like "as we will show in the paper, many bugs we found are too subtle to be discovered without examining concrete implementations."}

In this work, our goal is to \emph{test what is being executed}. We present a new methodology for testing legacy distributed systems and uncovering bugs before these systems are released in the wild. We achieve this using \psharp~\cite{deligiannis2015psharp}, an extension of the mainstream language \csharp that provides two key capabilities: (i) a flexible way of modeling the environment using simple language features; and (ii) a systematic concurrency testing framework that is able to capture and take control of all the nondeterminism in a real system (together with its modeled environment) and systematically explore execution paths to discover bugs. \SCComment{my comment is similar to the one in the intro: do we claim that we capture all sources of nondet?}

We present three case studies of using \psharp to test production distributed systems for Windows Azure inside Microsoft: a distributed storage management system and a live migration protocol. Using \psharp, we managed to uncover a very subtle bug that was haunting developers for a long time as they did not have an effective way to reproduce the bug and nail down the culprit. \psharp uncovered this bug in a very small setting, which made it easy to examine traces, identify and eventually fix the problem. We show that bugs in production do not need a large setting to be reproduced; they can be reproduced in small settings with easy to understand traces.

To summarize, our contributions are as follows:

\begin{itemize}
\item We present a methodology that allows flexible modeling of the environment of a distributed system using simple language mechanisms.
\item Our infrastructure can test production code written in \csharp, which is a mainstream language.
\item We present three case studies of using \psharp to test production distributed systems, finding bugs that could not be found with traditional testing techniques.
\item We show that we can reproduce bugs in production systems in a small setting with easy to understand traces.
\end{itemize}

\SCComment{let me give my overall comments about the current version of the paper here. This set of case studies are really cool. The fact that our approach is proven to be effective clearly is the primary selling point of the paper. Reviewers will love the results we have. 
About the writing, there may be 3 areas to improve:
(1) the word "we" is overloaded, because "we" play multiple roles in this work. "We" built \psharp; "we", as developers, designed the TableMigration system; "we", as testers, also take the responsibility to test the legacy Azure vNext system. The current writing doesn't clearly differentiate these roles, so readers may wonder how much work does it requires if they want to adopt the approach. 
(2) The intro makes me feel that we compare with existing logic reasoning approaches such as TLA+, and point out the limitation of TLA+. Until much later in the paper, I realize that our approach doesn't have any logic reasoning ingredient in it. I suggest that the intro makes it clear that our approach is complementary to TLA+, not a competing approach. We should avoid give a comparison with TLA+, because it is not an apple-to-apple comparison.
(3) Section 4 is the most important section about the approach. I feel that we spend too much text on section 4.3, which is about building the test harness. My concern is that readers' minds may be dominated by how we mocked all kinds of objects and override real methods by fake methods. They may feel that, in the end, we tested a highly simplified system. This is not true, because the extent manager is a REALLY complicated system (which is shown as a small box in fig.3). I suggest that we zoom into this system to explain why many things can go wrong and it is hard to test in a conventional manner.} 