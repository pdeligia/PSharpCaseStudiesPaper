Distributed systems are notoriously hard to design, implement and test~\cite{cavage2013there, laguna2015debugging, maddox2015test}. This is due to many well-known sources of \emph{nondeterminism}~\cite{chandra2007paxos}, such as race conditions in the asynchronous interaction between system components, the use of multithreaded code inside a component, unexpected node failures, unreliable communication channels and data losses, and interaction with (human) clients.
All these sources of nondeterminism translate into \emph{exponentially} many execution paths that a distributed system might potentially execute.
A bug might hide deep inside one of these paths and only manifest under extremely rare corner cases~\cite{gray1986computers, musuvathi2008finding}, but the consequence can be catastrophic~\cite{amazon2012aws, google2014outage}.

Developers of production distributed systems use many techniques to test their systems,
such as unit testing, integration testing, stress testing, and fault injection.
In spite of extensive use of these testing methods,
many bugs that arise from subtle combinations of concurrency and failure events
are missed during testing and get exposed only after a system has been put in production.
Discovering and fixing bugs in production, though, is bad for business as it can cost a lot of money~\cite{tassey2002economic} 
and many dissatisfied customers~\cite{amazon2012aws, google2014outage}.

We interviewed technical leaders and senior managers in the Microsoft Azure team regarding the top problems in distributed system development.
The consensus was that the most critical problem today is how to improve \emph{testing coverage} to find bugs \emph{before} a system goes in production.
The need for better testing techniques is not specific to Microsoft;
other companies, such as Amazon and Google, have acknowledged~\cite{chandra2007paxos,newcombe2015aws} that testing methodologies have to improve to be able to reason about the correctness of increasingly more complex distributed systems.

Arguably, the most advanced practice in industry so far appeals in Amazon Web Services where ``engineers use TLA+ to prevent serious but subtle bugs from reaching production''. The gist of this approach is to extract high level logic from production systems, represent the logic as specification in an expressive language (such as TLA+), and verify the specification using a model checker. While highly effective, as demonstrated by the numerous successes in AWS, this approach falls short of ``verifying that executable code correctly implements the high-level specification'' and the AWS team admits that ``we are not aware of any such tools that can handle distributed systems as large and complex as those being built at Amazon''~\ref{}.

In our opinion, high level specification is necessary but not sufficient, due to the gap between the specification and the executable code that ultimately powers the distributed systems. In this work, our goal is to bridge this gap with a new framework that allows verification techniques for high level specification directly applicable in the executable code. Instead of requiring developers to switch to unfamiliar special programming language~\ref{}, or manually annotate and instrument their code~\ref{}, we allow the developers to {\em test unmodified production code} by writing testing harness in our framework with familiar programming language. Based on our experiences, this significantly lowered acceptance barrier and was the key to the adoption of the framework in the Microsoft Azure team.

We present a new method for testing an implementation of a distributed system and uncovering bugs before the system is released in the wild.
We have implemented our testing method in \psharp~\cite{deligiannis2015psharp}, an extension of the mainstream language \csharp that provides
support for modeling, specification, and systematic testing of asynchronous systems.
To test a distributed system with the \psharp framework, the programmer augments the system code with three artifacts---a model of the nondeterministic execution environment of the system, a concurrent test harness that drives the system towards interesting behaviors, and safety or liveness specifications for each behavior of the system under the test harness.
The \psharp testing engine then systematically explores behaviors of the test harness and validates them against the provided specifications.

We applied our approach to test three production distributed storage systems implemented inside Microsoft for the Windows Azure cloud computing
platform: a distributed storage management system; a system for migrating data across partitions; and a big-data computing system.
We uncovered numerous bugs in these systems using our approach.
In particular, we uncovered a subtle liveness bug that was intermittently manifested during stress testing for months without being debugged.
Our testing approach uncovered this bug in a very small setting, which made it easy to examine traces, identify, and eventually fix the problem.
Our experience demonstrates that bugs in production do not need a large setting to be reproduced;
they can be reproduced in small settings with easy to understand traces.

To summarize, our contributions are as follows:
\begin{itemize}
\item We present a methodology that allows flexible modeling of the environment of a distributed system using simple language mechanisms.
\item Our infrastructure can test production code written in \csharp, which is a mainstream language.
\item We present three case studies of using \psharp to test production distributed systems, finding bugs that could not be found with traditional testing techniques.
\item We show that we can reproduce bugs in production systems in a small setting with easy to understand traces.
\end{itemize}

